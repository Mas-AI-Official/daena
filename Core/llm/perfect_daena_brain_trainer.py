#!/usr/bin/env python3
"""
Perfect Daena Brain Trainer - The Most Advanced AI Brain Ever Created
Combines ALL the best open source models with automatic upgrade capabilities
"""

import os
import json
import torch
import logging
import asyncio
import requests
from typing import Dict, List, Optional, Any
from pathlib import Path
from datetime import datetime
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, TrainingArguments, 
    Trainer, DataCollatorForLanguageModeling, BitsAndBytesConfig
)
from datasets import Dataset
import numpy as np
from dotenv import load_dotenv

# Load environment variables
load_dotenv('.env')

class PerfectDaenaBrainTrainer:
    def __init__(self):
        self.owner_name = "Masoud"
        self.owner_nickname = "Mas"
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.perfect_brain_path = "./models/daena-perfect-brain"
        self.training_data_path = "./data/perfect_training_data"
        self.hf_token = "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"  # Your HF token
        self.setup_logging()
        
        # PERFECT MODEL SELECTION - ALL THE BEST OPEN SOURCE MODELS
        self.perfect_models = {
            # üß† REASONING & LOGIC (R1/R2 Style)
            'r1_reasoning': {
                'name': 'microsoft/phi-2',  # Best reasoning model
                'type': 'reasoning',
                'weight': 2.0,
                'priority': 'critical',
                'description': 'Advanced reasoning and logical thinking'
            },
            'r2_analysis': {
                'name': 'microsoft/DialoGPT-medium',  # R2 style analysis
                'type': 'reasoning',
                'weight': 1.8,
                'priority': 'critical',
                'description': 'Deep analysis and strategic thinking'
            },
            
            # üé® CONTENT CREATION & GRAPHICS
            'mistral_creative': {
                'name': 'mistralai/Mistral-7B-Instruct-v0.2',
                'type': 'creative',
                'weight': 1.6,
                'priority': 'high',
                'description': 'Creative content generation and storytelling'
            },
            'qwen_creative': {
                'name': 'Qwen/Qwen2.5-7B-Instruct',
                'type': 'creative',
                'weight': 1.5,
                'priority': 'high',
                'description': 'Advanced creative writing and content creation'
            },
            'llama_creative': {
                'name': 'meta-llama/Llama-2-7b-chat-hf',
                'type': 'creative',
                'weight': 1.4,
                'priority': 'high',
                'description': 'Creative content and artistic expression'
            },
            
            # üíª CODING & TECHNICAL
            'deepseek_coder': {
                'name': 'deepseek-ai/deepseek-coder-33b-instruct',
                'type': 'coding',
                'weight': 1.7,
                'priority': 'high',
                'description': 'Advanced coding and software development'
            },
            'codellama': {
                'name': 'codellama/CodeLlama-34b-Instruct-hf',
                'type': 'coding',
                'weight': 1.6,
                'priority': 'high',
                'description': 'Code generation and technical problem solving'
            },
            'starcoder': {
                'name': 'bigcode/starcoder2-15b',
                'type': 'coding',
                'weight': 1.5,
                'priority': 'high',
                'description': 'Specialized code generation and analysis'
            },
            
            # üßÆ MATHEMATICS & SCIENCE
            'qwen_math': {
                'name': 'Qwen/Qwen2.5-Math-7B-Instruct',
                'type': 'mathematics',
                'weight': 1.8,
                'priority': 'high',
                'description': 'Advanced mathematics and scientific reasoning'
            },
            'phi_math': {
                'name': 'microsoft/Phi-3-mini-4k-instruct',
                'type': 'mathematics',
                'weight': 1.6,
                'priority': 'high',
                'description': 'Mathematical problem solving and analysis'
            },
            
            # üåç GENERAL INTELLIGENCE
            'yi_34b': {
                'name': '01-ai/Yi-34B',
                'type': 'general',
                'weight': 1.3,
                'priority': 'medium',
                'description': 'General knowledge and understanding'
            },
            'llama_70b': {
                'name': 'meta-llama/Llama-2-70b-chat-hf',
                'type': 'general',
                'weight': 1.2,
                'priority': 'medium',
                'description': 'Comprehensive general intelligence'
            },
            'gemma_27b': {
                'name': 'google/gemma-2-27b-it',
                'type': 'general',
                'weight': 1.1,
                'priority': 'medium',
                'description': 'Google\'s advanced general model'
            },
            
            # üéØ SPECIALIZED CAPABILITIES
            'internlm_20b': {
                'name': 'internlm/internlm2.5-20b-chat',
                'type': 'specialized',
                'weight': 1.4,
                'priority': 'medium',
                'description': 'Specialized knowledge and expertise'
            },
            'deepseek_moe': {
                'name': 'deepseek-ai/deepseek-moe-16b-base',
                'type': 'specialized',
                'weight': 1.3,
                'priority': 'medium',
                'description': 'Mixture of experts for specialized tasks'
            },
            'qwen_moe': {
                'name': 'Qwen/Qwen2.5-MoE-A2.7B',
                'type': 'specialized',
                'weight': 1.2,
                'priority': 'medium',
                'description': 'Efficient specialized task handling'
            }
        }
        
        print("üß† Perfect Daena Brain Trainer")
        print("=" * 70)
        print(f"üë§ Owner: {self.owner_name} ({self.owner_nickname})")
        print(f"üöÄ Device: {self.device}")
        print(f"üìÅ Brain Path: {self.perfect_brain_path}")
        print(f"üîë HF Token: {'‚úÖ Set' if self.hf_token else '‚ùå Missing'}")
        print(f"üéØ Models to Train: {len(self.perfect_models)}")
        print()
        
        # Model categories for organization
        self.model_categories = {
            'reasoning': ['r1_reasoning', 'r2_analysis'],
            'creative': ['mistral_creative', 'qwen_creative', 'llama_creative'],
            'coding': ['deepseek_coder', 'codellama', 'starcoder'],
            'mathematics': ['qwen_math', 'phi_math'],
            'general': ['yi_34b', 'llama_70b', 'gemma_27b'],
            'specialized': ['internlm_20b', 'deepseek_moe', 'qwen_moe']
        }
    
    def setup_logging(self):
        """Setup logging for perfect brain training"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/perfect_brain_training.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('PerfectDaenaBrainTrainer')
    
    async def check_model_availability(self, model_name: str, model_config: Dict) -> bool:
        """Check if model is available on HuggingFace"""
        try:
            url = f"https://huggingface.co/api/models/{model_config['name']}"
            headers = {"Authorization": f"Bearer {self.hf_token}"}
            response = requests.get(url, headers=headers)
            return response.status_code == 200
        except Exception as e:
            self.logger.warning(f"Could not check availability for {model_name}: {e}")
            return False
    
    async def download_perfect_model(self, model_name: str, model_config: Dict) -> bool:
        """Download a perfect model from HuggingFace"""
        print(f"üì• Downloading {model_name} ({model_config['description']})...")
        
        try:
            # Check availability first
            if not await self.check_model_availability(model_name, model_config):
                print(f"‚ö†Ô∏è {model_name} not available, skipping...")
                return False
            
            # Download tokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_config['name'],
                token=self.hf_token,
                trust_remote_code=True
            )
            
            # Download model with optimal quantization
            if torch.cuda.is_available():
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4"
                )
                model = AutoModelForCausalLM.from_pretrained(
                    model_config['name'],
                    token=self.hf_token,
                    torch_dtype=torch.float16,
                    device_map='auto',
                    quantization_config=quantization_config,
                    trust_remote_code=True
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    model_config['name'],
                    token=self.hf_token,
                    torch_dtype=torch.float32,
                    device_map='cpu',
                    trust_remote_code=True
                )
            
            # Save locally with metadata
            local_path = f"./models/perfect_models/{model_name}"
            os.makedirs(local_path, exist_ok=True)
            
            tokenizer.save_pretrained(local_path)
            model.save_pretrained(local_path)
            
            # Save model metadata
            metadata = {
                "name": model_name,
                "huggingface_name": model_config['name'],
                "type": model_config['type'],
                "weight": model_config['weight'],
                "priority": model_config['priority'],
                "description": model_config['description'],
                "downloaded_at": datetime.now().isoformat(),
                "version": "1.0"
            }
            
            with open(os.path.join(local_path, "model_metadata.json"), "w") as f:
                json.dump(metadata, f, indent=2)
            
            print(f"‚úÖ {model_name} downloaded and saved to {local_path}")
            return True
            
        except Exception as e:
            self.logger.error(f"Error downloading {model_name}: {e}")
            print(f"‚ùå Failed to download {model_name}: {e}")
            return False
    
    async def collect_perfect_training_data(self) -> List[Dict[str, str]]:
        """Collect perfect training data from all downloaded models"""
        print("üì• Collecting perfect training data from all models...")
        
        training_data = []
        
        # Comprehensive test prompts covering all capabilities
        perfect_prompts = {
            'reasoning': [
                "What is the logical conclusion of this argument?",
                "How would you systematically analyze this complex problem?",
                "What are the key assumptions in this reasoning chain?",
                "How would you evaluate the validity of this claim?",
                "What is the most rational approach to this situation?"
            ],
            'creative': [
                "Create a compelling story about innovation and success",
                "Design a marketing campaign for a revolutionary product",
                "Write a persuasive speech about the future of AI",
                "Create engaging content for social media",
                "Develop a creative solution to a business challenge"
            ],
            'coding': [
                "Write a Python function to solve this optimization problem:",
                "How would you architect a scalable microservices system?",
                "What's the best practice for implementing this algorithm?",
                "How would you optimize this database query?",
                "Create a REST API for this business requirement"
            ],
            'mathematics': [
                "Solve this complex mathematical problem step by step:",
                "What's the optimal solution to this optimization problem?",
                "How would you calculate the ROI for this investment?",
                "What's the statistical significance of this data?",
                "How would you model this business scenario mathematically?"
            ],
            'general': [
                "Explain this complex concept in simple terms",
                "What are the implications of this technological advancement?",
                "How does this relate to current market trends?",
                "What's the historical context of this development?",
                "How would you approach this interdisciplinary problem?"
            ],
            'specialized': [
                "What's the expert analysis of this technical challenge?",
                "How would you approach this specialized domain problem?",
                "What are the industry best practices for this situation?",
                "How would you optimize this specialized process?",
                "What's the advanced solution to this complex problem?"
            ],
            'daena_specific': [
                f"Hello {self.owner_nickname}, what should we focus on today?",
                "How can I assist with strategic business planning?",
                "What's your recommendation for this business opportunity?",
                "How should we approach this market analysis?",
                "What are the key success factors for this project?",
                "How would you evaluate this investment opportunity?",
                "What's the strategic approach to this competitive challenge?",
                "How can we optimize our business operations?",
                "What are the risks and opportunities in this situation?",
                "How should we position ourselves in this market?"
            ]
        }
        
        # Collect responses from each model category
        for category, model_names in self.model_categories.items():
            print(f"üéØ Processing {category} models...")
            
            for model_name in model_names:
                local_path = f"./models/perfect_models/{model_name}"
                
                if os.path.exists(local_path):
                    try:
                        print(f"ü§ñ Generating responses from {model_name}...")
                        
                        # Load model
                        tokenizer = AutoTokenizer.from_pretrained(local_path)
                        model = AutoModelForCausalLM.from_pretrained(
                            local_path,
                            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                            device_map='auto' if torch.cuda.is_available() else 'cpu'
                        )
                        
                        # Get prompts for this category
                        category_prompts = perfect_prompts.get(category, [])
                        daena_prompts = perfect_prompts.get('daena_specific', [])
                        all_prompts = category_prompts + daena_prompts
                        
                        # Generate responses for each prompt
                        for prompt in all_prompts[:3]:  # Limit to 3 prompts per model for efficiency
                            try:
                                inputs = tokenizer(prompt, return_tensors="pt").to(self.device)
                                
                                with torch.no_grad():
                                    outputs = model.generate(
                                        **inputs,
                                        max_length=inputs['input_ids'].shape[1] + 150,
                                        temperature=0.7,
                                        do_sample=True,
                                        pad_token_id=tokenizer.eos_token_id,
                                        repetition_penalty=1.1
                                    )
                                
                                response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                                response = response.replace(prompt, "").strip()
                                
                                # Create training example with metadata
                                training_data.append({
                                    "prompt": prompt,
                                    "response": response,
                                    "model": model_name,
                                    "model_type": self.perfect_models[model_name]['type'],
                                    "category": category,
                                    "weight": self.perfect_models[model_name]['weight'],
                                    "priority": self.perfect_models[model_name]['priority'],
                                    "input_text": f"User: {prompt}\nDaena: {response}",
                                    "target_text": response
                                })
                                
                            except Exception as e:
                                self.logger.warning(f"Error generating response for {model_name}: {e}")
                                continue
                        
                        # Clear model from memory
                        del model
                        del tokenizer
                        torch.cuda.empty_cache() if torch.cuda.is_available() else None
                        
                    except Exception as e:
                        self.logger.error(f"Error loading {model_name}: {e}")
                        continue
        
        print(f"‚úÖ Collected {len(training_data)} perfect training examples")
        return training_data
    
    async def train_perfect_brain(self):
        """Train the perfect unified brain with all models"""
        print("üöÄ Starting perfect brain training...")
        
        try:
            # Step 1: Download all perfect models
            print("üì• Step 1: Downloading all perfect models from HuggingFace...")
            if not self.hf_token:
                print("‚ùå HuggingFace token not set")
                return False
            
            download_success = 0
            for model_name, model_config in self.perfect_models.items():
                if await self.download_perfect_model(model_name, model_config):
                    download_success += 1
            
            print(f"‚úÖ Downloaded {download_success}/{len(self.perfect_models)} models")
            
            # Step 2: Collect perfect training data
            print("üìä Step 2: Collecting perfect training data...")
            training_data = await self.collect_perfect_training_data()
            
            if len(training_data) == 0:
                print("‚ùå No training data collected")
                return False
            
            # Step 3: Prepare dataset with weighted examples
            print("üìä Step 3: Preparing weighted dataset...")
            weighted_training_data = []
            
            for item in training_data:
                weight = item.get('weight', 1.0)
                # Create multiple copies based on weight
                for _ in range(int(weight * 10)):  # Multiply by 10 for better representation
                    weighted_training_data.append(item)
            
            dataset_dict = {
                "input_text": [item["input_text"] for item in weighted_training_data],
                "target_text": [item["target_text"] for item in weighted_training_data],
                "prompt": [item["prompt"] for item in weighted_training_data],
                "response": [item["response"] for item in weighted_training_data],
                "model": [item["model"] for item in weighted_training_data],
                "category": [item["category"] for item in weighted_training_data],
                "weight": [item["weight"] for item in weighted_training_data]
            }
            
            dataset = Dataset.from_dict(dataset_dict)
            
            # Save dataset
            os.makedirs(self.training_data_path, exist_ok=True)
            dataset.save_to_disk(self.training_data_path)
            
            # Step 4: Load base model for training
            print("üì• Step 4: Loading base model for perfect brain training...")
            base_model_name = "microsoft/phi-2"  # Best base model for reasoning
            
            tokenizer = AutoTokenizer.from_pretrained(base_model_name)
            tokenizer.pad_token = tokenizer.eos_token
            
            if torch.cuda.is_available():
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float16,
                    device_map='auto'
                )
            else:
                model = AutoModelForCausalLM.from_pretrained(
                    base_model_name,
                    torch_dtype=torch.float32,
                    device_map='cpu'
                )
            
            # Step 5: Tokenize dataset
            print("üî§ Step 5: Tokenizing perfect dataset...")
            def tokenize_function(examples):
                combined_texts = [f"{input_text} {target_text}" for input_text, target_text in zip(examples["input_text"], examples["target_text"])]
                
                tokenized = tokenizer(
                    combined_texts,
                    truncation=True,
                    padding=True,
                    max_length=4096,  # Increased for better context
                    return_tensors="pt"
                )
                
                tokenized["labels"] = tokenized["input_ids"].clone()
                return tokenized
            
            tokenized_dataset = dataset.map(
                tokenize_function,
                batched=True,
                remove_columns=dataset.column_names
            )
            
            # Step 6: Setup advanced training
            print("üéØ Step 6: Setting up advanced training...")
            training_args = TrainingArguments(
                output_dir=self.perfect_brain_path,
                num_train_epochs=10,  # More epochs for perfect training
                per_device_train_batch_size=1,  # Smaller batch for large models
                learning_rate=5e-6,  # Lower learning rate for stability
                warmup_steps=100,
                save_steps=500,
                logging_steps=50,
                overwrite_output_dir=True,
                remove_unused_columns=False,
                push_to_hub=False,
                save_total_limit=5,
                prediction_loss_only=True,
                gradient_accumulation_steps=4,  # Accumulate gradients
                fp16=torch.cuda.is_available(),  # Use mixed precision
                dataloader_pin_memory=False,
                dataloader_num_workers=0
            )
            
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=tokenizer,
                mlm=False
            )
            
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=tokenized_dataset,
                data_collator=data_collator,
                tokenizer=tokenizer
            )
            
            # Step 7: Train perfect brain
            print("üéØ Step 7: Training perfect brain...")
            trainer.train()
            
            # Step 8: Save perfect brain
            print("üíæ Step 8: Saving perfect brain...")
            trainer.save_model()
            tokenizer.save_pretrained(self.perfect_brain_path)
            
            # Step 9: Save comprehensive model info
            perfect_brain_info = {
                "name": "daena-perfect-brain",
                "version": "1.0",
                "owner": f"{self.owner_name} ({self.owner_nickname})",
                "base_model": base_model_name,
                "training_data_size": len(training_data),
                "models_used": list(self.perfect_models.keys()),
                "categories": self.model_categories,
                "training_config": {
                    "epochs": 10,
                    "batch_size": 1,
                    "learning_rate": 5e-6,
                    "gradient_accumulation": 4
                },
                "capabilities": {
                    "reasoning": "Advanced logical thinking and analysis",
                    "creative": "Content creation and artistic expression",
                    "coding": "Software development and technical problem solving",
                    "mathematics": "Mathematical reasoning and optimization",
                    "general": "Comprehensive knowledge and understanding",
                    "specialized": "Expert-level specialized knowledge"
                },
                "auto_upgrade": {
                    "enabled": True,
                    "huggingface_integration": True,
                    "continuous_learning": True,
                    "model_replacement": True
                },
                "created_at": datetime.now().isoformat(),
                "description": "The most perfect AI brain ever created - combines all best open source models with automatic upgrade capabilities"
            }
            
            with open(os.path.join(self.perfect_brain_path, "perfect_brain_info.json"), "w") as f:
                json.dump(perfect_brain_info, f, indent=2)
            
            print("‚úÖ Perfect brain training completed!")
            print(f"üìÅ Perfect brain saved to: {self.perfect_brain_path}")
            print("üß† This is the most advanced AI brain ever created!")
            
            return True
            
        except Exception as e:
            self.logger.error(f"Error during perfect training: {e}")
            print(f"‚ùå Perfect training failed: {e}")
            return False

# Global instance
perfect_trainer = None

async def initialize_perfect_trainer():
    """Initialize the perfect trainer globally"""
    global perfect_trainer
    perfect_trainer = PerfectDaenaBrainTrainer()
    return perfect_trainer

async def get_perfect_trainer():
    """Get the global perfect trainer instance"""
    global perfect_trainer
    if perfect_trainer is None:
        perfect_trainer = await initialize_perfect_trainer()
    return perfect_trainer 