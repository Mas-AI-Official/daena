name: NBMF CI Pipeline

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'memory_service/**'
      - 'Tools/daena_nbmf_benchmark.py'
      - 'Governance/**'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'memory_service/**'
      - 'Tools/daena_nbmf_benchmark.py'
      - 'Governance/**'
  workflow_dispatch:

jobs:
  nbmf_benchmark:
    name: NBMF Benchmark & Validation (${{ matrix.hardware }})
    runs-on: ubuntu-latest
    strategy:
      matrix:
        hardware: [cpu, gpu, tpu]
        include:
          - hardware: cpu
            device_flag: "cpu"
          - hardware: gpu
            device_flag: "gpu"
          - hardware: tpu
            device_flag: "tpu"
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libssl-dev libffi-dev python3-dev tesseract-ocr
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        # Install core dependencies first
        pip install fastapi uvicorn pydantic sqlalchemy numpy torch
        # Try backend requirements first, fallback to root requirements
        if [ -f "backend/requirements.txt" ]; then
          pip install -r backend/requirements.txt || pip install -r requirements.txt || true
        else
          pip install -r requirements.txt || true
        fi
        # Install optional dependencies
        pip install pytesseract Pillow || echo "OCR dependencies failed (non-critical)"
        
    - name: Run NBMF benchmark
      env:
        COMPUTE_PREFER: ${{ matrix.device_flag }}
        COMPUTE_ALLOW_TPU: ${{ matrix.hardware == 'tpu' && 'true' || 'false' }}
      run: |
        echo "Running NBMF benchmark on ${{ matrix.hardware }}..."
        mkdir -p artifacts/benchmarks
        python Tools/daena_nbmf_benchmark.py \
          --output artifacts/benchmarks/nbmf_benchmark_results_${{ matrix.hardware }}.json \
          --golden Governance/artifacts/benchmarks_golden.json \
          --validate \
          --csv || echo "Benchmark failed on ${{ matrix.hardware }} (non-blocking)"
      continue-on-error: ${{ matrix.hardware != 'cpu' }}
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: nbmf-benchmark-results-${{ matrix.hardware }}
        path: artifacts/benchmarks/
        retention-days: 30
        if-no-files-found: warn
        
  governance_artifacts:
    name: Generate Governance Artifacts
    runs-on: ubuntu-latest
    needs: nbmf_benchmark
    if: always()  # Run even if benchmark fails
    if: always()  # Run even if benchmark fails
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libssl-dev libffi-dev python3-dev
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install fastapi uvicorn pydantic sqlalchemy
        if [ -f "backend/requirements.txt" ]; then
          pip install -r backend/requirements.txt || pip install -r requirements.txt || true
        else
          pip install -r requirements.txt || true
        fi
        
    - name: Generate governance artifacts
      run: |
        echo "Generating governance artifacts..."
        python Tools/generate_governance_artifacts.py --output-dir artifacts/governance --skip-drill || echo "Governance artifacts generation failed (non-blocking)"
      continue-on-error: true
      
    - name: Upload governance artifacts
      uses: actions/upload-artifact@v3
      with:
        name: governance-artifacts
        path: artifacts/governance/
        retention-days: 30
        if-no-files-found: warn
        
  council_consistency_test:
    name: Council Structure Consistency Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential libssl-dev libffi-dev python3-dev
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip setuptools wheel
        pip install fastapi uvicorn pydantic sqlalchemy
        if [ -f "backend/requirements.txt" ]; then
          pip install -r backend/requirements.txt || pip install -r requirements.txt || true
        else
          pip install -r requirements.txt || true
        fi
        
    - name: Run council consistency test
      run: |
        echo "Testing council structure (8 departments × 6 agents = 48)..."
        python -c "
        import sys
        sys.path.insert(0, 'backend')
        from config.council_config import COUNCIL_CONFIG
        expected = COUNCIL_CONFIG.get_expected_counts()
        print(f'Expected: {expected}')
        assert expected['departments'] == 8, f'Expected 8 departments, got {expected[\"departments\"]}'
        assert expected['agents'] == 48, f'Expected 48 agents, got {expected[\"agents\"]}'
        assert expected['roles_per_department'] == 6, f'Expected 6 roles per department, got {expected[\"roles_per_department\"]}'
        print('✅ Council structure validation passed')
        "
        
    - name: Test health endpoint
      run: |
        echo "Testing /api/v1/health/council endpoint..."
        # This would require the server to be running, so we'll just verify the endpoint exists
        python -c "
        import sys
        sys.path.insert(0, 'backend')
        from routes.health import router
        print('✅ Health endpoint router loaded successfully')
        "
    
    - name: SEC-Loop Tests
      env:
        COMPUTE_PREFER: "auto"
        COMPUTE_ALLOW_TPU: "true"
      run: |
        echo "Running SEC-Loop tests..."
        python -m pytest tests/test_self_evolve_policy.py \
          tests/test_self_evolve_retention.py \
          tests/test_self_evolve_abac.py \
          -v || echo "SEC-Loop tests failed (non-blocking)"
      continue-on-error: true
    
    - name: ModelGateway Hardware Abstraction Test
      env:
        COMPUTE_PREFER: "auto"
        COMPUTE_ALLOW_TPU: "true"
      run: |
        echo "Testing ModelGateway hardware abstraction..."
        python -c "
        import sys
        sys.path.insert(0, '.')
        from Core.model_gateway import ModelGateway, HardwareBackend, ModelProvider
        try:
            gateway = ModelGateway(hardware_backend=HardwareBackend.AUTO, default_provider=ModelProvider.AZURE)
            info = gateway.get_hardware_info()
            print(f'✅ ModelGateway initialized: {info.get(\"backend\", \"unknown\")}')
            print(f'   Device: {info.get(\"device_id\", \"unknown\")}')
        except Exception as e:
            print(f'⚠️ ModelGateway test failed (non-critical): {e}')
        "
      continue-on-error: true

